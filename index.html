<style>
a:link {text-decoration:none}
a:visited {text-decoration:none}
a:active {text-decoration:none}
a:hover {text-decoration:none; color:#990000}
body {background-color:#f0f0f0}

.section {margin:10px; max-width:960px}

td {padding:10px}

li {margin:10px}
</style>
<head>
<title>Ashok Cutkosky</title>
</head>


<div class="section">
<h2>Ashok Cutkosky</h2>
<table>
    <tr>
        <td>
        <img src="pic/ashokcutkosky_pic1.jpg" height="200">
        </td>
        <td>
    I am an assistant professor at Boston University in the ECE department since the fall of 2020. 
    <br>
    Previously, I was a research scientist at Google.
    I earned a PhD in computer science at Stanford
  University in 2018 under the supervision of <a href=http://web.stanford.edu/group/brainsinsilicon/>Kwabena Boahen</a>, and a AB in mathematics from Harvard in 2013. I am also a <a
  href=https://web.stanford.edu/dept/registrar/bulletin1112/5353.htm>master of medicine</a>. 
  <br>
  I'm currently excited about optimization algorithms for machine learning. I have recently worked on non-convex optimization as well as adaptive online learning.
<br>
My email address is ashok (at) cutkosky (dot) com.
<br>
<br>
<!-- <b>I am looking for faculty positions! <a href=jobs/CV.pdf>[CV]</a> <a href=jobs/researchstatement.pdf>[Research Statement]</a> <a href=jobs/teachingstatement.pdf>[Teaching Statement]</a> <a href=jobs/diversitystatement.pdf>[Diversity Statement]</a> -->
</td>
</tr>
</table>
<br>
<br>
<h2>Selected Publications</h2>
<ul>
    <li><a href=https://arxiv.org/abs/2302.03775>Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion</a>, Ashok Cutkosky, Harsh Mehta and Francesco Orabona. International Conference on Machine Learning (ICML), 2023.</li>
    <li><a href=https://openreview.net/pdf?id=uhKtQMn21D>Mechanic: A Learning Rate Tuner</a>, Ashok Cutkosky, Aaron Defazio and Harsh Mehta. Neural Information Processsing Systems (NeurIPS), 2023.</li>
    <li><a href=https://arxiv.org/abs/1905.10018>Momentum-Based Variance Reduction in Non-Convex SGD</a>, Ashok Cutkosky and Francesco Orabona. Neural Information Processsing Systems (NeurIPS), 2019.</li>
    <li><a href=https://arxiv.org/abs/1903.00974>Anytime Online-to-Batch, Optimism, and Acceleration</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2019.</li>
    <li><a href=https://arxiv.org/abs/1802.06293>Black-Box Reductions for Parameter-free Online Learning in Banach Spaces</a>, Ashok Cutkosky and Francesco Orabona, Conference on Learning Theory (COLT), 2018</li>
    <li><a href=papers/CUTKOSKY17.pdf>Online Learning Without Prior Information</a>, Ashok Cutkosky and Kwabena Boahen, Conference on Learning Theory (COLT), 2017</li>
</ul>
<h2>Current Students</h2>
<ul>
    <li>Hoang Tran</li>
    <li><a href=https://ajjacobs.github.io/homepage/>Andrew Jacobsen</a></li>
    <li>Jiujia Zhang</li>
    <li>Qinzi Zhang</li>
    <li>Elly Wang</li>
    <li>Peter Gu</li>
</ul>
<h2>Past Students (PhD)</h2>
<ul>
    <li><a href=https://zhiyuzz.github.io/>Zhiyu Zhang</a></li>
</ul>
<h2>Past Students (undergraduate)</h2>
<ul>
    <li>Fangrui Huang</li>
    <li>Rana Boustany</li>
    <li>Michelle Zyman</li>
    <li>Vance Raiti</li>
</ul>
<h2>Teaching</h2>
<ul>
    <li>Spring 2023:  EC414 Introduction to Machine Learning</li>
    <li>2021-2023: EC525 Optimization for Machine Learning (<a href=https://optmlclass.github.io>website</a>)</li>
    <li>Fall 2020: EC524 Deep Learning (co-taught with Brian Kulis)</li>
</ul>
<br>
<h2>All Publications</h2>
In 2023 I achieved my stretch-goal for academic productivity: I became too lazy to keep this list updated regularly. If it looks out of date, please check out my <a href=https://scholar.google.com/citations?user=h4AbGp0AAAAJ&hl=en>google scholar profile</a>. Try sorting by year rather than citation to see what I've been working on recently.
<ul>
    <li><a href=https://openreview.net/attachment?id=uhKtQMn21D&name=pdf>Mechanic: a Learning Rate Tuner</a>, Ashok Cutkosky, Aaron Defazio, Harsh Mehta. Neural Information Processing Systems (NeurIPS) 2023.</li>
    <li><a href=https://openreview.net/pdf?id=NBMIsOS6B7>Alternation makes the adversary weaker in two-player games</a>, Volkan Cevher, Ashok Cutkosky, Ali Kavis, Georgios Piliouras, Stratis Skoulakis, Luca Viano. Neural Information Processing Systems (NeurIPS) 2023.</li>
    <li><a href=https://arxiv.org/abs/2301.13349>Unconstrained dynamic regret via sparse coding</a>, Zhiyu Zhang, Ashok Cutkosky, Ioannis Ch Paschalidis. Neural Information Processing Systems (NeurIPS) 2023.</li>
    <li><a href=https://openreview.net/pdf?id=SgeIqUvo4w>Bandit Online Linear Optimization with Hints and Queries</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit. International Conference on Machine Learning (ICML) 2023.</li>
    <li><a href=https://openreview.net/pdf?id=2K2vEVBm5G>Unconstrained Online Learning with Unbounded Losses</a>, Andrew Jacobsen, Ashok Cutkosky. International Conference on Machine Learning (ICML) 2023.</li>
    <li><a href=https://openreview.net/pdf?id=GimajxXNc0>Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion</a>, Ashok Cutkosky, Harsh Mehta, Francesco Orabona. International Conference on Machine Learning (ICML) 2023.</li>   
    <li><a href=https://proceedings.mlr.press/v216/cutkosky23a/cutkosky23a.pdf>Blackbox optimization of unimodal functions</a>, Ashok Cutkosky, Abhimanyu Das, Weihao Kong, Chansoo Lee, Rajat Sen. Uncertainty in Artificial Intelligence (UAI) 2023.</li>
    <li><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/972cd27c994a806e187ef1c2f5254059-Paper-Conference.pdf>Optimal Comparator Adaptive Online Learning with Switching Cost</a>, Zhiyu Zhang, Ashok Cutkosky, Yannis Paschalidis. Neural Information Processing Systems (NeurIPS) 2022.</li>
    <li><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/349956dee974cfdcbbb2d06afad5dd4a-Paper-Conference.pdf>Parameter-free regret in high probability with heavy tails</a>, Jiujia Zhang, Ashok Cutkosky. Neural Information Processing Systems (NeurIPS) 2022.</li>
    <li><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/47547ee84e3fbbcbbbbad7c1fd9a973b-Paper-Conference.pdf>Momentum aggregation for private non-convex erm</a>, Hoang Tran, Ashok Cutkosky. International Conference on Machine Learning (ICML) 2023.</li>
    <li><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/1704fe7aaff33a54802b83a016050ab8-Paper-Conference.pdf>Better sgd using second-order momentum</a>, Hoang Tran, Ashok Cutkosky. Neural Information Processing Systems (NeurIPS) 2022.</li>
    <li><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/d1422213c9f2bdd5178b77d166fba86a-Supplemental-Conference.pdf>Differentially Private Online-to-Batch for Smooth Losses</a>, Qinzi Zhang, Hoang Tran, Ashok Cutkosky. Neural Information Processing Systems (NeurIPS) 2022.</li>
    <li><a href=https://arxiv.org/abs/2201.07877>PDE-Based Optimal Strategy for Unconstrained Online Learning</a>, Zhiyu Zhang, Ashok Cutkosky, and Yannis Paschalidis. International Conference on Machine Learning (ICML) 2022.</li>
    <li><a href=https://arxiv.org/abs/2203.00444>Parameter-Free Mirror Descent</a>, Andrew Jacobsen and Ashok Cutkosky. Conference on Learning Theory (COLT) 2022.</li>
    <li><a href=https://arxiv.org/abs/2203.04274>Leveraging Initial Hints for Free in Stochastic Linear Bandits</a>, Ashok Cutkosky, Chris Dann, Abhimanyu Das, Qiuyi (Richard) Zhang. International Conference on Algorithmic Learning Theory (ALT) 2022.</li>
    <li><a href=https://arxiv.org/abs/2203.10327>Implicit Parameter-free Online Learning with Truncated Linear Models</a>, Keyi Chen, Ashok Cutkosky, Francesco Orabona. International Conference on Algorithmic Learning Theory (ALT) 2022.</li>
    <li><a href=https://arxiv.org/abs/2102.01623>Adversarial Tracking Control via Strongly Adaptive Online Learning with Memory</a>, Zhiyu Zhang, Ashok Cutkosky, Yannis Paschalidis. International Conference on Artificial Intillgence and Statistics (AISTATS) 2022.</li>
    <li><a href=https://arxiv.org/abs/2106.14343>High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails</a>, Ashok Cutkosky, Harsh Mehta. Neural Information Processing Systems (NeurIPS) 2021. [oral]</li>
    <li><a href=https://arxiv.org/abs/2111.05257>Logarithmic Regret from Sublinear Hints</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit. Neural Information Processing Systems (NeurIPS) 2021.</li>
    <li><a href=https://arxiv.org/abs/2110.14243>Online Selective Classification with Limited Feedback</a>, Aditya Gangrade, Anil Kag, Ashok Cutkosky, Venkatesh Saligrama. Neural Information Processing Systems (NeurIPS) 2021. [spotlight]</li>
    <li><a href=https://proceedings.mlr.press/v139/cutkosky21a.html>Dynamic Balancing for Model Selection in Bandits and RL</a>, Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, Manish Purohit. International Conference on Machine Learning (ICML) 2021.</li>
    <li><a href=https://proceedings.mlr.press/v139/alieva21a.html>Robust Pure Exploration in Linear Bandits with Limited Budget</a>, Ayya Alieva, Ashok Cutkosky, Abhimanyu Das. International Conference on Machine Learning (ICML) 2021.</li>
    <li><a href=http://proceedings.mlr.press/v130/bhaskara21b.html>Power of Hints for Online Learning with Movement Costs</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit. International Conference on Artificial Intillgence and Statistics (AISTATS) 2021.</li>
    <li><a href=https://openreview.net/pdf?id=Z4R1vxLbRLO>Extreme Memorization via Scale of  Initialization</a>. Harsh Mehta, Ashok Cutkosky, Benham Neyshabur. International Conference on Learning Representations (ICLR) 2021.</li>
    <li><a href=https://arxiv.org/abs/2010.03082>Online Linear Optimization with Many Hints</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit. Advances Neural Information Processing Systems (NeurIPS), 2020.</li>
    <li><a href=https://arxiv.org/abs/2007.08448>Comparator-Adaptive Convex Bandits</a>, Dirk van der Hoeven, Ashok Cutkosky, Haipeng Luo. Advances in Neural Information Processing Systems (NeurIPS), 2020.</li>
    <li><a href=papers/varyingnormbounds.pdf>Better Full-Matrix Regret via Parameter-free Online Learning</a>, Ashok Cutkosky. Advances in Neural Information Processing Systems (NeurIPS), 2020.</li>
    <li><a href=https://arxiv.org/abs/2002.03305>Momentum Improves Normalized SGD</a>, Ashok Cutkosky and Harsh Mehta. International Conference on Machine Learning (ICML), 2020.</li>
    <li><a href=http://arxiv.org/abs/2002.04726>Online Learning with Imperfect Hints</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. International Conference on Machine Learning (ICML), 2020</li>
    <li><a href=http://proceedings.mlr.press/v119/cutkosky20a.html>Parameter-Free, Dynamic, and Strongly-Adaptive Online Learning</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2020</li>
    <li><a href=https://arxiv.org/abs/1905.10018>Momentum-Based Variance Reduction in Non-Convex SGD</a>, Ashok Cutkosky and Francesco Orabona. Neural Information Processsing Systems (NeurIPS), 2019.</li>
    <li><a href=https://arxiv.org/abs/1905.10680>Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration</a>, Kwang-Sung Jun, Ashok Cutkosky, Francesco Orabona. Neural Information Processing Systems (NeurIPS), 2019.</li>
    <li><a href=https://arxiv.org/abs/1905.12721>Matrix-Free Preconditioning in Online Learning</a>, Ashok Cutkosky and Tamas Sarlos. International Conference on Machine Learning (ICML), 2019. <a href=https://github.com/google-research/google-research/tree/master/recursive_optimizer>[code]</a> [long talk]</li>
    <li><a href=https://arxiv.org/abs/1903.00974>Anytime Online-to-Batch, Optimism, and Acceleration</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2019. [long talk]</li>
    <li><a href=https://arxiv.org/abs/1901.09068>Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization</a>, Zhenxun Zhuang, Ashok Cutkosky, and Francesco Orabona. International Conference on Machine Learning (ICML), 2019.</li>
    <li><a href=https://arxiv.org/abs/1902.09013>Artificial Constraints and Hints for Unbounded Online Learning</a>, Ashok Cutkosky. Conference on Learning Theory (COLT), 2019.</li>
    <li><a href=https://arxiv.org/abs/1902.09003>Combining Online Learning Guarantees</a>, Ashok Cutkosky. Conference on Learning Theory (COLT), 2019.</li>
    <li><a href=https://arxiv.org/abs/1802.05811>Distributed Stochastic Optimization via Adaptive Stochastic Gradient Descent</a>, Ashok Cutkosky and Robert Busa-Fekete. Advances in Neural Information Processing Systems (NeurIPS) 2018.</li>
    <li><a href=https://arxiv.org/abs/1802.06293>Black-Box Reductions for Parameter-free Online Learning in Banach Spaces</a>, Ashok Cutkosky and Francesco Orabona, Conference on Learning Theory (COLT), 2018</li>
    <li><a href=https://proceedings.neurips.cc/paper/2017/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf>Stochastic and Adversarial Online Learning Without Hyperparameters</a>, Ashok Cutkosky and Kwabena Boahen, Advances in Neural Information Processing Systems (NIPS), 2017.</li>
    <li><a href=http://proceedings.mlr.press/v65/cutkosky17a/cutkosky17a.pdf>Online Learning Without Prior Information</a>, Ashok Cutkosky and Kwabena Boahen, Conference on Learning Theory (COLT), 2017 <b>[Best Student Paper Award]</b>. <a href=https://github.com/acutkosky/freerex>[code]</a></li>
    <li><a href=https://proceedings.neurips.cc/paper_files/paper/2016/file/550a141f12de6341fba65b0ad0433500-Paper.pdf>Online Convex Optimization with Unconstrained Domains and Losses</a>, Ashok
    Cutkosky and Kwabena Boahen, Advances in Neural Information Processing Systems (NIPS), 2016. <a href=https://www.youtube.com/watch?v=9e4yhhI_1yc>[magical video]</a></li>
    <li><a href=ashok.cutkosky.com/papers/bloomfeatures.pdf>Bloom Features</a>, Ashok Cutkosky and Kwabena
    Boahen, IEEE International Conference on
    Computation Science and Computational Intelligence, 2015.</li>
    <li><a href=http://www.pnas.org/content/112/47/E6456.full.pdf?with-ds=yes>Chromatin extrusion explains key features of loop and domain formation in wild-type and engineered genomes</a>, Adrian Sanborn et al, Proceedings of the National Academy of Sciences, 2015.</li>
</ul>
<br>
<h3>Other</h3>
<ul>
    <li><a href=papers/thesis.pdf>PhD dissertation: Algorithms and Lower Bounds for Parameter-Free Online Learning</a>, May 2018</li>
    <li><a href=papers/polymerthesis.pdf>Undergrad Thesis: Polymer Simulations and DNA Topology</a>, Advised by <a href=http://www.aidenlab.org/>Erez Lieberman Aiden</a>. [Hoopes Prize and Captain Jonathan Fay Prize winner]</li>
    <li><a href=https://arxiv.org/abs/0901.1678>Associated Primes of the Square of the Alexander Dual of Hypergraphs</a>, high school math project. Advised by <a href=https://math.okstate.edu/people/chris/>Chris Francisco</a>.
</ul>
</div>
