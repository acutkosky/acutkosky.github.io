<style>
a:link {text-decoration:none}
a:visited {text-decoration:none}
a:active {text-decoration:none}
a:hover {text-decoration:none; color:#990000}
body {background-color:#f0f0f0}

.section {margin:10px; max-width:960px}

td {padding:10px}

li {margin:10px}
</style>
<head>
<title>Ashok Cutkosky</title>
</head>


<div class="section">
<h2>Ashok Cutkosky</h2>
<table>
    <tr>
        <td>
        <img src="pic/ashokcutkosky_pic1.jpg" height="200">
        </td>
        <td>
    I joined Boston University as an assistant professor in the ECE department in the Fall of 2020. Previously, I was a research scientist at Google.
    I earned a PhD in computer science at Stanford
  University in 2018, and am also a graduate of the <a
  href=http://msm.stanford.edu/>Masters in Medicine</a> program. I'm currently
excited about optimization algorithms for machine learning. I have recently worked on non-convex optimization as well as adaptive online learning.
<br>
My advisor was <a href=http://web.stanford.edu/group/brainsinsilicon/>Kwabena Boahen</a>.
<br>
My email address is ashok (at) cutkosky (dot) com.
<br>
<br>
<!-- <b>I am looking for faculty positions! <a href=jobs/CV.pdf>[CV]</a> <a href=jobs/researchstatement.pdf>[Research Statement]</a> <a href=jobs/teachingstatement.pdf>[Teaching Statement]</a> <a href=jobs/diversitystatement.pdf>[Diversity Statement]</a> -->
</td>
</tr>
</table>
<br>
<h2>Selected Publications</h2>
<ul>
    <li><a href=https://arxiv.org/abs/2002.03305>Momentum Improves Normalized SGD</a>, Ashok Cutkosky and Harsh Mehta. International Conference on Machine Learning (ICML), 2020.</li>
Learning</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2020</li>
    <li><a href=https://arxiv.org/abs/1905.10018>Momentum-Based Variance Reduction in Non-Convex SGD</a>, Ashok Cutkosky and Francesco Orabona. Neural Information Processsing Systems (NeurIPS), 2019.</li>
    <li><a href=https://arxiv.org/abs/1903.00974>Anytime Online-to-Batch, Optimism, and Acceleration</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2019.</li>
    <li><a href=https://arxiv.org/abs/1802.06293>Black-Box Reductions for Parameter-free Online Learning in Banach Spaces</a>, Ashok Cutkosky and Francesco Orabona, Conference on Learning Theory (COLT), 2018</li>
</ul>
<br>
<br>
<h2>All Publications</h2>
<ul>
    <li>Power of Hints for Online Learning with Momvement Costs, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit. To appear in International Conference on Artificial Intillgence and Statistics (AISTATS) 2021.</li>
    <li>Extreme Memorization via Scale of  Initialization. Harsh Mehta, Ashok Cutkosky, Benham Neyshabur. To appear in International Conference on Learning Representations (ICLR) 2021.</li>
    <li><a href=https://arxiv.org/abs/2010.03082>Online Linear Optimization with Many Hints</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit. Advances Neural Information Processing Systems (NeurIPS), 2020.</li>
    <li><a href=https://arxiv.org/abs/2007.08448>Comparator-Adaptive Convex Bandits</a>, Dirk van der Hoeven, Ashok Cutkosky, Haipeng Luo. Advances in Neural Information Processing Systems (NeurIPS), 2020.</li>
    <li><a href=papers/varyingnormbounds.pdf>Better Full-Matrix Regret via Parameter-free Online Learning</a>, Ashok Cutkosky. Advances in Neural Information Processing Systems (NeurIPS), 2020.</li>
    <li><a href=https://arxiv.org/abs/2002.03305>Momentum Improves Normalized SGD</a>, Ashok Cutkosky and Harsh Mehta. International Conference on Machine Learning (ICML), 2020.</li>
    <li><a href=http://arxiv.org/abs/2002.04726>Online Learning with Imperfect Hints</a>, Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. International Conference on Machine Learning (ICML), 2020</li>
    <li><a href=http://proceedings.mlr.press/v119/cutkosky20a.html>Parameter-Free, Dynamic, and Strongly-Adaptive Online Learning</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2020</li>
    <li><a href=https://arxiv.org/abs/1905.10018>Momentum-Based Variance Reduction in Non-Convex SGD</a>, Ashok Cutkosky and Francesco Orabona. Neural Information Processsing Systems (NeurIPS), 2019.</li>
    <li><a href=https://arxiv.org/abs/1905.10680>Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration</a>, Kwang-Sung Jun, Ashok Cutkosky, Francesco Orabona. Neural Information Processing Systems (NeurIPS), 2019.</li>
    <li><a href=https://arxiv.org/abs/1905.12721>Matrix-Free Preconditioning in Online Learning</a>, Ashok Cutkosky and Tamas Sarlos. International Conference on Machine Learning (ICML), 2019. <a href=https://github.com/google-research/google-research/tree/master/recursive_optimizer>[code]</a></li>
    <li><a href=https://arxiv.org/abs/1903.00974>Anytime Online-to-Batch, Optimism, and Acceleration</a>, Ashok Cutkosky. International Conference on Machine Learning (ICML), 2019.</li>
    <li><a href=https://arxiv.org/abs/1901.09068>Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization</a>, Zhenxun Zhuang, Ashok Cutkosky, and Francesco Orabona. International Conference on Machine Learning (ICML), 2019.</li>
    <li><a href=https://arxiv.org/abs/1902.09013>Artificial Constraints and Hints for Unbounded Online Learning</a>, Ashok Cutkosky. Conference on Learning Theory (COLT), 2019.</li>
    <li><a href=https://arxiv.org/abs/1902.09003>Combining Online Learning Guarantees</a>, Ashok Cutkosky. Conference on Learning Theory (COLT), 2019.</li>
    <li><a href=https://arxiv.org/abs/1802.05811>Distributed Stochastic Optimization via Adaptive Stochastic Gradient Descent</a>, Ashok Cutkosky and Robert Busa-Fekete. Advances in Neural Information Processing Systems (NeurIPS) 2018.</li>
    <li><a href=https://arxiv.org/abs/1802.06293>Black-Box Reductions for Parameter-free Online Learning in Banach Spaces</a>, Ashok Cutkosky and Francesco Orabona, Conference on Learning Theory (COLT), 2018</li>
    <li><a href=papers/onlinealigned.pdf>Stochastic and Adversial Online Learning Without Hyperparameters</a>, Ashok Cutkosky and Kwabena Boahen, Advances in Neural Information Processing Systems (NIPS), 2017.</li>
    <li><a href=papers/CUTKOSKY17.pdf>Online Learning Without Prior Information</a>, Ashok Cutkosky and Kwabena Boahen, Conference on Learning Theory (COLT), 2017 <b>[Best Student Paper Award]</b>. <a href=https://github.com/acutkosky/freerex>[code]</a></li>
    <li><a href=papers/unconstrainedOCO.pdf>Online Convex Optimization with Unconstrained Domains and Losses</a>, Ashok
    Cutkosky and Kwabena Boahen, Advances in Neural Information Processing Systems (NIPS), 2016. <a href=https://www.youtube.com/watch?v=9e4yhhI_1yc>[magical video]</a></li>
    <li><a href=papers/bloomfeatures.pdf>Bloom Features</a>, Ashok Cutkosky and Kwabena
    Boahen, IEEE International Conference on
    Computation Science and Computational Intelligence, 2015.</li>
    <li><a href=http://www.pnas.org/content/112/47/E6456.full.pdf?with-ds=yes>Chromatin extrusion explains key features of loop and domain formation in wild-type and engineered genomes</a>, Adrian Sanborn et al, Proceedings of the National Academy of Sciences, 2015.</li>
    <li><a href=papers/polymerthesis.pdf>Polymer Simulations and DNA Topology</a>, Undergraduate thesis at Harvard University. Won Hoopes Prize and Captain Jonathan Fay Prize for most outstanding and original undergraduate thesis. Advised by <a href=http://www.aidenlab.org/>Erez Lieberman Aiden</a>.</li>
    <li><a href=https://arxiv.org/abs/0901.1678>Associated Primes of the Square of the Alexander Dual of Hypergraphs</a>, project for the Siemens Competition in Math Science and Technology in high school. Won 5th place nationally. Advised by <a href=https://math.okstate.edu/people/chris/>Chris Francisco</a>.
</ul>
<br>
<h2>Doctoral Dissertation</h2>
<ul>
    <li><a href=papers/thesis.pdf>Algorithms and Lower Bounds for Parameter-Free Online Learning</a>, May 2018</li>
</ul>
</div>
